[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Dive the Singleton as an excuse to study Instantiation in Python\n\n\n\n\n\n\n\npython\n\n\ndesign pattern\n\n\nmetaclasses\n\n\nmultithreading\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nx0s\n\n\n\n\n\n\n\n\nHow to fluently forward Makefile’s targets to a Docker container ?\n\n\n\n\n\n\n\ncli\n\n\nmakefile\n\n\nbash\n\n\ndocker\n\n\ncontainer\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2023\n\n\nx0s\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A blog written by a Machine Learning Engineer who enjoys setting the pipes in line with data to deliver value in meaningful projects."
  },
  {
    "objectID": "posts/2023-08-11_singleton-instantiation/article.html",
    "href": "posts/2023-08-11_singleton-instantiation/article.html",
    "title": "Dive the Singleton as an excuse to study Instantiation in Python",
    "section": "",
    "text": "In this post, we will construct and deconstruct the Singleton Pattern in order to hone our understanding of object instanciation in Python (through Inheritance, Metaprogramming and Multithreading).\nfrom wikipedia: In software engineering, the singleton pattern is a software design pattern that restricts the instantiation of a class to a singular instance.\nAs an example, we will try to define as a singleton a class named Logger representing a logging component for an application. As we usually want a logger to be a global ressource for documenting what an application is actually processing.\nWe will make multiple attempts building successively on the previous one depending on the problems encountered. Here is a summary on the methods we will cover :\n\nAttempt 1: Inheritance Overriding\nAttempt 2: Metaclass Calling\nAttempt 3: Multithreads Racing\nAttempt 4: Multithreads Deadlocking\n\nBefore attempting, let’s write a helper function to check if an object is a singleton or not:\n\ndef is_singleton(cls: type) -> None:\n    print(f\"{cls.__name__} {'is' if cls() is cls() else 'is not'} a Singleton\")\n\n\n\n\nWe define the Singleton as a base class and control instanciation through its __new__ method. The classes we want to make unique will then inherit from it.\nIn Python, the static __new__ method is responsible for object creation. It seems a good place to force any instanciation to return a unique object. Note that the object returned by __new__ is the one passed to __init__ as self. Let’s try:\n\nclass Singleton:\n    _instance = None\n\n    def __new__(cls, *args, **kwargs):\n        # If the instance being created(cls) is not already created and cached (cls._instance)\n        if not isinstance(cls._instance, cls):\n            cls._instance = super().__new__(cls, *args, **kwargs)\n        return cls._instance        \n\nclass Logger(Singleton): pass\n\n# Let's verify a singleton is generated\nis_singleton(Logger)\n\nLogger is a Singleton\n\n\nIt seems to work, but we may encounter some difficulties that may break the singleton:\n\nRisk 1: Reversing Bases\n\nPython’s Method Resolution Order operates from left to right, Child classes have to first inherit from Singleton to benefit from its __new__ method: class Logger(Singleton, LoggerBase). Reversing order may neutralize the Singleton (if LoggerBase defines its own __new__ method)\n\nRisk 2: Overriding __new__\n\nIf a child class overrides its __new__ method, it redefines instanciation and may produce non-singleton objects\n\n\nLet’s see:\n\nclass LoggerBase: \n    def __new__(cls, *args, **kwargs):\n        # Overriding instance creation method __new__\n        return object.__new__(cls, *args, **kwargs)\n\nclass Logger(Singleton, LoggerBase): pass\n\n# Risk 1: Reversing Bases\nclass LoggerReversed(LoggerBase, Singleton): pass\n\n# Risk 2: Overriding __new__\nclass LoggerChild(Logger):\n    def __new__(cls, *args, **kwargs):\n        return object.__new__(cls, *args, **kwargs)\n\nfor LoggerClass in (Logger, LoggerReversed, LoggerChild):\n    is_singleton(LoggerClass)\n\nLogger is a Singleton\nLoggerReversed is not a Singleton\nLoggerChild is not a Singleton\n\n\nIt is a bit annoying that our Singleton can be broken so easily. But in Python, Metaclasses are responsible for Class creation(like class of a class). We will see in next attempt how to use metaclasses to control instantiation upstream to inheritance.\n\n\n\nBy default the type Metaclass is responsible for class creation. When we instantiate a new object, the Metaclass’ __call__ method is in fact called before the Class’ __new__ method ! Here is the proof:\n\nclass MetaTest(type):\n    def __call__(cls, *args, **kwargs):\n        print(\"In MetaTest.__call__\")\n        return super().__call__(*args, **kwargs)\n\nclass Test(metaclass=MetaTest):\n    def __new__(cls, *args, **kwargs):\n        print(\"In Test.__new__\")\n        return super().__new__(cls, *args, **kwargs)\n    \n    def __init__(self, *args, **kwargs):\n        print(\"In Test.__init__\")\n\n_ = Test()\n\nIn MetaTest.__call__\nIn Test.__new__\nIn Test.__init__\n\n\nThe builtin type.__call__ is called line 4, is in charge of:\n\ncalling Test.__new__ to create a Test object (allocating memory for it)\ncalling Test.__init__ to initialize the newly created object only if\n\nTest.__new__ has previously returned an object of type Test and\nTest.__init__ is defined\n\n\nThen, it seems a good choice to control instantiation from within the Metaclass __call__ to enforce the singleton and neutralize any child overriding. Let’s try:\n\n# Solution 1 (not thread safe)\nclass Singleton(type):\n    _instances = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n        return cls._instances[cls]\n\nclass LoggerBase: \n    def __new__(cls, *args, **kwargs):\n        # Overriding instance creation method __new__\n        return object.__new__(cls, *args, **kwargs)\n\nclass Logger(LoggerBase, metaclass=Singleton): pass\n\n# Overriding __new__ as before\nclass LoggerChild(Logger):\n    def __new__(cls, *args, **kwargs):\n        return object.__new__(cls, *args, **kwargs)\n\nis_singleton(Logger)\nis_singleton(LoggerChild)\n\nLogger is a Singleton\nLoggerChild is a Singleton\n\n\nSo far, so good. Finally, we found a solution that works on a singe thread, but what would happen if we instantiate the Logger on multiple Threads ?\n\n\n\nFrom Wikipedia: “Thread safe: Implementation is guaranteed to be free of race conditions when accessed by multiple threads simultaneously”\nIn Python, there is a Global Interpreter Lock(GIL) that gives a lot of stability in Python programs by allowing only one thread at a time to operate on the interpreter. Still the GIL can be passed between threads if no other lock is held. (More on GIL & Multithreading)\nNote: Multiprocessing is not limited by the GIL, because different interpreters are spawned, but we won’t talk about this in this post. because it is only possible to share states and not instances of any class between processes, it becomes really convoluted trying to enforce the singleton pattern.\nSo, If we instantiate the same Class on multiple Threads, it is possible that one or more threads reads nearly the same time that no instance has been created:\nclass Singleton(type):\n    _instances = {}\n\n    def __call__(cls, *args, **kwargs):\n        # At the same moment, Thread B is checking the control-flow\n        # Since cls._instances is still empty, it will pass too\n        # and instantiate a second occurence of the class\n        # breaking the singleton\n        if cls not in cls._instances:\n            # Thread A just passed the control flow and starts\n            # instantiating but cls._instances is still empty\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n        return cls._instances[cls]\nTo mitigate this, we introduce a mutex, called Lock that will be aquired by only one thread at a time. The Lock will be released after the instance creation and registration in cls._instances. This way another Thread could acquire it but surely with the register up-to-date:\nclass SingletonThreadSafe(type):\n    _instances = {}\n    _lock = Lock()\n\n    def __call__(cls, *args, **kwargs):\n        # Thread B is waiting here\n        with cls._lock:\n            if cls not in cls._instances:\n                # Thread A acquired the Lock, passed the control-flow\n                # and now is instantiating the object and registering it\n                # to cls._instances\n                cls._instances[cls] = super().__call__(*args, **kwargs)\n        # Thread A releases the Lock. Then Thread B acquires it\n        # but won't pass the control-flow, since an instance\n        # has already been registered. It will return the existing one\n        return cls._instances[cls]\nLet’s check with the following script our claims:\n\n%%writefile thread_safety_test.py\n\nfrom multiprocessing.dummy import Pool as ThreadPool\nfrom threading import get_ident, Lock\n\n\nclass Singleton(type):\n    _instances = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n        return cls._instances[cls]\n\n\nclass SingletonThreadSafe(type):\n    _instances = {}\n    _lock = Lock()\n\n    def __call__(cls, *args, **kwargs):\n        with cls._lock:\n            if cls not in cls._instances:\n                cls._instances[cls] = super().__call__(*args, **kwargs)\n        return cls._instances[cls]\n\n\nclass LoggerBase: \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        print(f\"\\nInstantiated {self.__class__.__name__} in thread {get_ident()}\")\n\nclass Logger(LoggerBase, metaclass=Singleton): pass\nclass LoggerThreadSafe(LoggerBase, metaclass=SingletonThreadSafe): pass\n\n\ndef execute(LoggerClass: LoggerBase) -> None:\n    \"\"\"Try to instantiate LoggerClass on multiple Threads multiple times\"\"\"\n    with ThreadPool(7) as pool:\n        [pool.apply_async(LoggerClass) for _ in range(100)]\n        pool.close()\n        pool.join()\n\n\nif __name__ == \"__main__\":\n    execute(Logger)\n    print(\"-\" * 30)\n    execute(LoggerThreadSafe)\n\nOverwriting thread_safety_test.py\n\n\n\n!python thread_safety_test.py\n\n\nInstantiated Logger in thread 22300\nInstantiated Logger in thread 8128\nInstantiated Logger in thread 23812\n\n\n------------------------------\n\nInstantiated LoggerThreadSafe in thread 16288\n\n\nYes, we found a version fo the singleton that is Thread-safe.\nIn multithreading, most of the time, the instance will be already registered. For efficiency and minimizing race condition, instead of Looking(LBYL) Before You Leap(LBYL), we may find it Easier to Ask for Forgiveness than Permission (EAFP). Trying to directly return the instance should be quicker than checking first if it is available:\nclass SingletonThreadSafe(type):\n    _instances = {}\n    _lock = Lock()\n\n    def __call__(cls, *args, **kwargs):\n        with cls._lock:\n            # Let's try to return the instance\n            try:\n                return cls._instances[cls]\n            except KeyError:\n                # if not available, ok, let's create it\n                cls._instances[cls] = super().__call__(*args, **kwargs)\n                return cls._instances[cls]\n\n\n\nFrom Wikipedia: In concurrent computing, deadlock is any situation in which no member of some group of entities can proceed because each waits for another member, including itself, to take action, such as sending a message or, more commonly, releasing a lock\nWhat would happen if we define two singletons that are connected together ?\nSo far we instantiated the Lock within the metaclass:\nclass SingletonThreadSafe(type):\n    _instances = {}\n    _lock = Lock()\n    ...\nThen the same Lock is shared between classes. If at least two of these classes are also coupled in their initlization, we would have a deadlock, and the program will loop undefinitely, like in the following example:\n\nclass Logger(metaclass=SingletonThreadSafe): pass\n        \nclass DBConnection(metaclass=SingletonThreadSafe):\n    def __init__(self):\n        # When we initialize DBConnection, we also instantiate\n        # a logger, but the lock will never be released since\n        # DBConnection already aqcuired it\n        self.logger = Logger()\n\n\nis_singleton(Logger)\n# is_singleton(DBConnection) # will deadlock \n\nLogger is a Singleton\n\n\n\nclass SingletonDeadlockFree(type):\n    \"\"\"Each class using this  Singleton metaclass has its own Lock,\n    preventing them from deadlock\"\"\"\n    _instances = {}\n\n    def __new__(metacls, cls, bases, clsdict, *args, **kwargs):\n        \"\"\"The lock is set at class definition to avoid deadlocking between coupled classes\"\"\"\n        cls_with_lock = super().__new__(metacls, cls, bases, clsdict)\n        cls_with_lock._lock = Lock()\n        return cls_with_lock\n\n    def __call__(cls, *args, **kwargs):\n        with cls._lock:\n            if cls not in cls._instances:\n                cls._instances[cls] = super().__call__(*args, **kwargs)\n        return cls._instances[cls]\n\nLet’s check if that works:\n\nclass Logger(metaclass=SingletonDeadlockFree): pass\n        \nclass DBConnection(metaclass=SingletonDeadlockFree):\n    def __init__(self):\n        self.logger = Logger()\n\nis_singleton(Logger)\nis_singleton(DBConnection)\n\nLogger is a Singleton\nDBConnection is a Singleton\n\n\n\n\n\nWe’ve seen a lot of ways to implement a Singleton in Python. Whats mattered the most was the journey and questioning the data model in Python and how we can be sure we access the object we want in single or multithreaded environments."
  },
  {
    "objectID": "posts/2023-03-16_makefile-wrapper/article.html",
    "href": "posts/2023-03-16_makefile-wrapper/article.html",
    "title": "How to fluently forward Makefile’s targets to a Docker container ?",
    "section": "",
    "text": "We will see how to painlessly redirect makefile’s targets to a docker container keeping the API intuitive.\nWhat you will learn:\n\nHow to automatically read a Makefile’s targets from another Makefile\nHow to wrap Makefile targets to a docker container\nHow to extract and pass Makefile arguments to another Makefile"
  },
  {
    "objectID": "posts/2023-03-16_makefile-wrapper/article.html#introducing-a-use-case",
    "href": "posts/2023-03-16_makefile-wrapper/article.html#introducing-a-use-case",
    "title": "How to fluently forward Makefile’s targets to a Docker container ?",
    "section": "Introducing a use case",
    "text": "Introducing a use case\nLet’s say we’ve already built an application using Makefile to run tasks (ie: run tests, build a docker image, launch scripts). As good practice we developed it within an virtual environment (ie: conda) and it worked perfectly… until one day… as the application grew, it had to deal with different versions of python, compiler dependencies… things that a conda environment cannot handle alone. So we decided to wrap your app in a container.\nTo sum up, from being able to do:\n(my_env) $ make test\n(my_env) $ make game WHEN=2022/01-2\nwe also want to be able to do:\n(my_env) $ make_in_container test\n(my_env) $ make_in_container game WHEN=2022/01-2\nWhy choosing make_in_container ?\n\nWe don’t want to duplicate Makefiles nor the targets (DRY)\nWe favour clarity, avoiding mingling arguments, prefering make_in_container test_this DAY=2022/01 to make test_this SKIP_CONTAINER=false DAY=2022/01. Also with this option we would have to manually prefix every command of the original Makefile\nWe also favour extensibility, what if later we want to switch from Docker to Podman ? make_in_container looks better than make_in_docker\n\nTo illustrate this, we will use the git repository I worked on to do the famous Advent Of Code ! So we have this tree structure:\nAdvent-Of-Code\n├── Dockerfile\n├── Makefile\n├── Makefile_docker\n├── advent_of_code/\n├── tests/\n| ...\nThere are two makefiles:\n\na Makefile having the project usual targets (install,build, test …)\na Makefile_docker aiming at forwarding the targets to a docker container.\n\nThe workflow is the following:\n# Install the application following the guidelines\n# - Create conda environment\n# - make install\n# - ...\n\n# Build the docker image\n(my_env) $ make build\n\n# Run some tasks in container\n(my_env) $ make_in_container test_this DAY=2022/01\nOnce the image is built, a user should be able to run any task within the container as soon as the conda environment is active. We’ll see in next sessions how to settle this."
  },
  {
    "objectID": "posts/2023-03-16_makefile-wrapper/article.html#aliasing-make_in_container",
    "href": "posts/2023-03-16_makefile-wrapper/article.html#aliasing-make_in_container",
    "title": "How to fluently forward Makefile’s targets to a Docker container ?",
    "section": "Aliasing make_in_container",
    "text": "Aliasing make_in_container\nTo avoid polluting system namespace (sourcing ~/.bashrc), we want to enable the make_in_container command only when working in this project. Enabling it when we activate the conda environment seems the right moment. Let’s configure it at building time:\n\n# from Makefile\nbuild:\n    @docker build --tag aoc-image -f Dockerfile .\n# make \"make_in_container\" command available when conda env is activated\nifdef CONDA_PREFIX\n    @$(eval PATH_ALIAS := ${CONDA_PREFIX}/etc/conda/activate.d/aliases_.sh)\n    @mkdir -p ${CONDA_PREFIX}/etc/conda/activate.d\n    @echo \"#!/bin/bash\" >> $(PATH_ALIAS)\n    @echo \"alias make_in_container='make -f Makefile_docker'\" >> $(PATH_ALIAS)\n    @source $(PATH_ALIAS)\nendif\n\nFirst, we build and image named aoc-image using Dockerfile. Then if a conda environment is active (and it should!), we write a little script this conda env will execute every time it is activated. Therefore make_in_container will redirect to using Makefile_docker as a makefile:\n\n#!/bin/bash\nalias make_in_container='make -f Makefile_docker'\n\nOf course, you can adapt this code to make it work with other environment manager (Virtualenvwrapper hooks for instance)."
  },
  {
    "objectID": "posts/2023-03-16_makefile-wrapper/article.html#how-to-retrieve-all-targets",
    "href": "posts/2023-03-16_makefile-wrapper/article.html#how-to-retrieve-all-targets",
    "title": "How to fluently forward Makefile’s targets to a Docker container ?",
    "section": "How to retrieve all targets ?",
    "text": "How to retrieve all targets ?\nHere we want to automatically forward targets to the docker container. First we retrieve the targets from Makefile using a snippet heavily inspired from StackOverflow. We can see the result by invoking make_in_container list-targets.\n\n# from Makefile_docker\n\n# Tag of docker image\nNAME := aoc-image\n\n# Retrieve the AOC targets from main Makefile\n# inspired from https://stackoverflow.com/a/26339924/3581903\nAOC_TARGETS := $(shell LC_ALL=C make -pRrq -f Makefile : 2>/dev/null \\\n        | awk -v RS= -F: '/(^|\\n)\\# Files(\\n|$$)/,/(^|\\n)\\# Finished Make data base/ {if ($$1 !~ \"^[\\#.]\") {print $$1}}' \\\n        | sort | egrep -v -e '^[^[:alnum:]]' -e '^$@$$' \\\n        | xargs | tr -d :)\n\n.PHONY : $(AOC_TARGETS) list_targets\n\n# Forward any AOC_TARGET to the container\n$(AOC_TARGETS):\n    @docker run -it ${NAME} make $@\n\nlist-targets:\n    @echo $(AOC_TARGETS)\n\nWith this setting, calls like make_in_container help or make_in_container test properly forward make help and make test in the container. But what about make_in_container test_this DAY=2022/01 ? For now it only forwards make_in_container test_this. Let’s see how to pass all arguments to the container."
  },
  {
    "objectID": "posts/2023-03-16_makefile-wrapper/article.html#how-to-pass-all-arguments",
    "href": "posts/2023-03-16_makefile-wrapper/article.html#how-to-pass-all-arguments",
    "title": "How to fluently forward Makefile’s targets to a Docker container ?",
    "section": "How to pass all arguments ?",
    "text": "How to pass all arguments ?\nAs per the manual, $@ only keeps the target name. Calling make_in_container test_this DAY=2022/01 will set $@ to test_this and DAY to 2022/01. To retrieve the argument, we have to check the environment variables. Here we will store the arguments’ names in AOC_ARGS (it is possible to retrieve it automatically with regexp on the Makefile help, but it would add unecessary complexity here).\nFor each argument, get_args will check if an environment variable is set (like DAY to 2022/01), and when it is, will return the string DAY=2022/01.\n\n# Extract valid arguments and pass them with their value\n# ie: calling \"make_in_container game WHEN=2022/01-1\" will result in passing \"WHEN=2022/01-1\"\nget_args = $(foreach arg,$(AOC_ARGS),$(if $(value $(arg)),$(arg)=$($(arg))))\n\n# Arguments to be passed to targets accordin to main Makefile\nAOC_ARGS = EDIT TOKEN WHEN VERBOSE DAY\n\nTherefore, we update the redirection so it can forward the arguments to the container. make_in_container test_this DAY=2022/01 will be forwarded as make test_this DAY=2022/01.\n\n# Forward any AOC_TARGET to the container   \n# ie: \"make_in_container test VERBOSE=1\" is run as \"make test VERBOSE=1\" in the container\n$(AOC_TARGETS):\n    @docker run -it ${NAME} make $@ $(call get_args)\n\nYou can reach the full source here."
  },
  {
    "objectID": "posts/2023-03-16_makefile-wrapper/article.html#conclusion",
    "href": "posts/2023-03-16_makefile-wrapper/article.html#conclusion",
    "title": "How to fluently forward Makefile’s targets to a Docker container ?",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve seen how to wrap all makefile targets so they can be forwarded to a container without changing the API: Everything that was runnable with make ... is now also runnable in a container with make_in_container ...."
  }
]